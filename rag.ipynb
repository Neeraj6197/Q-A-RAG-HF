{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries:\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'docs\\\\Databricks-Big-Book-Of-GenAI-FINAL.pdf', 'page': 1}, page_content='THE BIG BOOK OF GENERATIVE AICONTENTSIntroduction  ............................................................................................................................................................................................................ 3\\nThe Path to Deploying Production-Quality GenAI Applications  ............................................................................................. 5')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the pdf from the folder:\n",
    "loader = PyPDFDirectoryLoader(\"./docs\")\n",
    "documents = loader.load()\n",
    "\n",
    "#splitting into chunks:\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "final_document = text_splitter.split_documents(documents)\n",
    "final_document[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal Projects\\Q&A RAG using HUGGINGFACE\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Personal Projects\\Q&A RAG using HUGGINGFACE\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NeerajNagrajJain\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#initializing embedding technique:\n",
    "hugging_face_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the vector store:\n",
    "vector_store = FAISS.from_documents(final_document[:100],hugging_face_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What Is DBRX?\n",
      "DBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token \n",
      "prediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which \n",
      "36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data. Compared \n",
      "to other open MoE models like Mixtral and Grok-1, DBRX is fine-grained, meaning it uses a larger number of\n"
     ]
    }
   ],
   "source": [
    "#Query using similar search:\n",
    "query = \"What is DBRX?\"\n",
    "relevant_documents = vector_store.similarity_search(query)\n",
    "print(relevant_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C9592D48F0>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a retriever object:\n",
    "retriever = vector_store.as_retriever(search_type='similarity',\n",
    "                                      search_kwargs={\"k\":3})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the huggingface api key:\n",
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading a hugging face model:\n",
    "llm = HuggingFaceHub(\n",
    "        repo_id = \"mistralai/Mistral-7B-v0.1\",\n",
    "        model_kwargs={\"temperature\":0.1,\n",
    "                      \"max_length\":500}              \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a prompt template:\n",
    "template = '''\n",
    "use the following context to answer the questions asked.\n",
    "{context}\n",
    "Question:{question}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(template=template,\n",
    "                        input_variables=[\"context\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a retireval QA:\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\":prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "use the following context to answer the questions asked.\n",
      "of models we have built and brought to production with our customers.\n",
      "To build DBRX, we leveraged the same suite of Databricks tools that are available to our customers. We \n",
      "managed and governed our training data using Unity Catalog. We explored this data using newly acquired  \n",
      "Lilac AI . We processed and cleaned this data using Apache Sparkâ„¢ and Databricks notebooks. We trained \n",
      "DBRX using optimized versions of our open-source training libraries: MegaBlocks , LLM Foundry , Composer ,\n",
      "\n",
      "THE BIG BOOK OF GENERATIVE AIThe weights of the base model ( DBRX Base ) and the fine-tuned model ( DBRX Instruct ) are available on Hugging \n",
      "Face under an open license. Starting today, DBRX is available for Databricks customers to use via APIs, and \n",
      "Databricks customers can pretrain their own DBRX-class models from scratch or continue training on top of  \n",
      "one of our checkpoints using the same tools and science we used to build it. DBRX is already being integrated\n",
      "\n",
      "example of the powerful and efficient models being built at Databricks for a wide range of applications, from \n",
      "internal features to ambitious use-cases for our customers.\n",
      "As with any new model, the journey with DBRX is just the beginning, and the best work will be done by those \n",
      "who build on it: enterprises and the open community. This is also just the beginning of our work on DBRX, and \n",
      "you should expect much more to come.\n",
      "Contributions\n",
      "Question:How to Get Started With DBRX on Databricks\n",
      "Answer:\n",
      "\n",
      "Question:How to Get Started With DBRX on Databricks\n",
      "Answer:\n",
      "\n",
      "Question:How to Get Started With DBRX on Databricks\n",
      "Answer:\n",
      "\n",
      "Question:How to Get Started With DBRX on Databricks\n",
      "Answer:\n",
      "\n",
      "Question:How to Get Started With DBRX on Databricks\n",
      "Answer:\n",
      "\n",
      "Question:How to Get Started With DBRX on\n"
     ]
    }
   ],
   "source": [
    "#testing the model with a query:\n",
    "query = \"How to Get Started With DBRX on Databricks\"\n",
    "response = retrievalQA.invoke({\"query\":query})\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
